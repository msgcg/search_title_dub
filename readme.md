# Поиск дубликатов Title-тегов на сайте (SEO-аудит)

Это Python-скрипт для проведения базового SEO-аудита сайта. Его основная задача — просканировать указанный сайт и найти страницы с одинаковыми мета-тегами `<title>`. Скрипт демонстрирует принципы веб-краулинга и парсинга HTML.

## Почему дубликаты Title — это проблема?

Мета-тег `<title>` — один из важнейших факторов ранжирования для поисковых систем (Яндекс, Google). Дублирование этого тега на разных страницах сайта является серьезной технической ошибкой:

1.  **Размытие релевантности:** Поисковик не может определить, какая из страниц с одинаковым заголовком является наиболее релевантной для запроса пользователя.
2.  **Каннибализация ключевых слов:** Страницы начинают конкурировать друг с другом в поисковой выдаче, ослабляя позиции обеих.
3.  **Ухудшение поведенческих факторов:** Пользователи видят одинаковые заголовки в результатах поиска и не понимают, какая страница им нужна, что может снизить кликабельность (CTR).

## Основные возможности скрипта

*   **Рекурсивный обход сайта:** Начиная со стартовой страницы, скрипт переходит по всем внутренним ссылкам.
*   **Извлечение `Title`:** На каждой странице парсится HTML-код и извлекается содержимое тега `<title>`.
*   **Эффективная нормализация URL:** Чтобы не сканировать одну и ту же страницу дважды, скрипт приводит URL к единому виду (убирает `http/https`, `www` и слэш в конце).
*   **Игнорирование файлов:** Скрипт не пытается скачать и обработать ссылки, ведущие на файлы (jpg, png, pdf, docx и т.д.).
*   **Подробное логирование:** В консоль выводится информация о текущем обрабатываемом URL и процессе поиска новых ссылок.
*   **Ограничение сканирования:** Для предотвращения бесконечного цикла и слишком долгой работы установлен лимит на количество сканируемых страниц.
*   **Итоговый отчет:** По завершении работы скрипт выводит список дублирующихся заголовков и URL-адресов, на которых они были найдены.

## Требования

*   Python 3.x
*   Библиотека `requests`

Для установки зависимостей выполните команду:
```bash
pip install requests
```

## Как использовать

1.  Клонируйте репозиторий:
    ```bash
    git clone https://github.com/msgcg/search_title_dub.git
    ```
2.  Установите зависимости (см. выше).
3.  В самом конце файла измените стартовый URL на адрес сайта, который вы хотите проверить:
    ```python
    if __name__ == '__main__':
        # Замените 'http://example.com/' на URL целевого сайта
        duplicate_titles = find_duplicate_titles_improved('http://example.com/')
    ```
4.  Запустите скрипт из терминала:
    ```bash
    python search_title.py
    ```
5.  Наблюдайте за процессом сканирования в консоли. По окончании работы скрипт выведет итоговый отчет.

## Пример работы и вывода

При запуске скрипта для сайта `example.com` может быть получен следующий результат, который точно указывает на технические ошибки сайта:

```
--- Сканирование завершено ---
Затрачено времени: 125.42 секунд
Всего обработано URL: 200
Всего найдено уникальных Title: 75

--- Найдены дубликаты мета-тегов Title: ---

Title: 'О компании — Example Inc.'
  - http://example.com/about
  - http://example.com/about.html

Title: 'Наша специальная услуга — Example Inc.'
  - http://example.com/promo/special-service
  - http://example.com/services/special-service
```
Этот отчет показывает, что на сайте есть две страницы "О компании" с разными URL, но одинаковым контентом, что требует настройки 301-го редиректа. Также он демонстрирует, как одна и та же услуга может быть доступна по разным адресам (например, в разделе "Акции" и "Услуги"), что также является ошибкой.

## Как работает код

Скрипт имитирует поведение поискового робота:

1.  **Инициализация:** Создается очередь URL-адресов для посещения (`urls_to_visit`) и множество для уже посещенных (нормализованных) URL (`visited_normalized_urls`), чтобы избежать повторных проверок.
2.  **Цикл сканирования:** Пока очередь не пуста и не достигнут лимит, скрипт берет первый URL из очереди.
3.  **Загрузка и парсинг:** С помощью `requests` загружается HTML-код страницы. С помощью регулярного выражения (`re`) из кода извлекается содержимое тега `<title>`.
4.  **Сбор данных:** Найденный заголовок и URL сохраняются в словарь.
5.  **Поиск новых ссылок:** В HTML-коде находятся все теги `<a href="...">`. Каждая ссылка очищается, нормализуется и, если она ведет на этот же сайт и еще не была посещена, добавляется в очередь.
6.  **Финальный отчет:** После завершения цикла скрипт анализирует собранные данные и выводит те заголовки, которым соответствует более одного URL.

**Возможные доработки:**
*   Переход на асинхронный код (`asyncio` + `aiohttp`) для ускорения сканирования.
*   Интеграция с `Selenium` или `Playwright` для поддержки JavaScript.
*   Добавление экспорта результатов в CSV-файл для удобного анализа.
*   Реализация учета правил из файла `robots.txt`.